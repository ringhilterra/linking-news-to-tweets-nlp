{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will read in all the cleaned tweet .csv files provided and write them to postgres tables, (1 per file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it assumes you have unzipped the ProcessedTweetDataFeb file in this directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy as sq\n",
    "import io\n",
    "import re\n",
    "pd.options.display.max_rows\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ringhilterra/virtualenvs/py3dev/lib/python3.6/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "PG_CONN_STRING = '***'\n",
    "engine = sq.create_engine(PG_CONN_STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_pg(result_df, tweet_table):\n",
    "    schema = 'ryan_test'\n",
    "    result_df.head(0).to_sql(tweet_table, con=engine, schema=schema, index=False, if_exists='replace') #truncates the table\n",
    "    conn = engine.raw_connection()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\"SET search_path TO '{schema}'\".format(schema=schema))\n",
    "    output = io.StringIO()\n",
    "    result_df.to_csv(output, sep='\\t', header=False, index=False)\n",
    "    output.seek(0)\n",
    "    contents = output.getvalue()\n",
    "    cur.copy_from(output, table=tweet_table, null=\"\") # null values become ''\n",
    "    conn.commit()\n",
    "    print('completed write to table: {0}'.format(tweet_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through the tweet file directory and get all the file paths\n",
    "file_list = []\n",
    "day_dirs = os.listdir('ProcessedTweetDataFeb/')\n",
    "for day_dir in day_dirs:\n",
    "    #print(day_dir)\n",
    "    if day_dir[0] != '.':\n",
    "        fnames = os.listdir('ProcessedTweetDataFeb/'+day_dir)\n",
    "        fnames = [('ProcessedTweetDataFeb/' + day_dir + '/' + x) for x in fnames]\n",
    "        file_list = file_list + fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed write to table: tweetsprocessed2018_02_01_1\n",
      "completed write to table: tweetsprocessed2018_02_01_2\n",
      "completed write to table: tweetsprocessed2018_02_01_3\n",
      "completed write to table: tweetsprocessed2018_02_01_4\n",
      "completed write to table: tweetsprocessed2018_02_01_5\n",
      "completed write to table: tweetsprocessed2018_02_01_6\n",
      "completed write to table: tweetsprocessed2018_02_01_7\n",
      "completed write to table: tweetsprocessed2018_02_10_1\n",
      "completed write to table: tweetsprocessed2018_02_10_2\n",
      "completed write to table: tweetsprocessed2018_02_10_3\n",
      "completed write to table: tweetsprocessed2018_02_10_4\n",
      "completed write to table: tweetsprocessed2018_02_10_5\n",
      "completed write to table: tweetsprocessed2018_02_10_6\n",
      "completed write to table: tweetsprocessed2018_02_10_7\n",
      "completed write to table: tweetsprocessed2018_02_10_8\n",
      "completed write to table: tweetsprocessed2018_02_02_1\n",
      "completed write to table: tweetsprocessed2018_02_02_10\n",
      "completed write to table: tweetsprocessed2018_02_02_11\n",
      "completed write to table: tweetsprocessed2018_02_02_12\n",
      "completed write to table: tweetsprocessed2018_02_02_13\n",
      "completed write to table: tweetsprocessed2018_02_02_2\n",
      "completed write to table: tweetsprocessed2018_02_02_3\n",
      "completed write to table: tweetsprocessed2018_02_02_4\n",
      "completed write to table: tweetsprocessed2018_02_02_5\n",
      "completed write to table: tweetsprocessed2018_02_02_6\n",
      "completed write to table: tweetsprocessed2018_02_02_7\n",
      "completed write to table: tweetsprocessed2018_02_02_8\n",
      "completed write to table: tweetsprocessed2018_02_02_9\n",
      "completed write to table: tweetsprocessed2018_02_03_1\n",
      "completed write to table: tweetsprocessed2018_02_03_2\n",
      "completed write to table: tweetsprocessed2018_02_03_3\n",
      "completed write to table: tweetsprocessed2018_02_03_4\n",
      "completed write to table: tweetsprocessed2018_02_03_5\n",
      "completed write to table: tweetsprocessed2018_02_03_6\n",
      "completed write to table: tweetsprocessed2018_02_03_7\n",
      "completed write to table: tweetsprocessed2018_02_03_8\n",
      "completed write to table: tweetsprocessed2018_02_04_1\n",
      "completed write to table: tweetsprocessed2018_02_04_2\n",
      "completed write to table: tweetsprocessed2018_02_04_3\n",
      "completed write to table: tweetsprocessed2018_02_04_4\n",
      "completed write to table: tweetsprocessed2018_02_04_5\n",
      "completed write to table: tweetsprocessed2018_02_04_6\n",
      "completed write to table: tweetsprocessed2018_02_04_7\n",
      "completed write to table: tweetsprocessed2018_02_04_8\n",
      "completed write to table: tweetsprocessed2018_02_04_9\n",
      "completed write to table: tweetsprocessed2018_02_05_1\n",
      "completed write to table: tweetsprocessed2018_02_05_10\n",
      "completed write to table: tweetsprocessed2018_02_05_11\n",
      "completed write to table: tweetsprocessed2018_02_05_2\n",
      "completed write to table: tweetsprocessed2018_02_05_3\n",
      "completed write to table: tweetsprocessed2018_02_05_4\n",
      "completed write to table: tweetsprocessed2018_02_05_5\n",
      "completed write to table: tweetsprocessed2018_02_05_6\n",
      "completed write to table: tweetsprocessed2018_02_05_7\n",
      "completed write to table: tweetsprocessed2018_02_05_8\n",
      "completed write to table: tweetsprocessed2018_02_05_9\n",
      "completed write to table: tweetsprocessed2018_02_06_1\n",
      "completed write to table: tweetsprocessed2018_02_06_2\n",
      "completed write to table: tweetsprocessed2018_02_06_3\n",
      "completed write to table: tweetsprocessed2018_02_06_4\n",
      "completed write to table: tweetsprocessed2018_02_06_5\n",
      "completed write to table: tweetsprocessed2018_02_06_6\n",
      "completed write to table: tweetsprocessed2018_02_06_7\n",
      "completed write to table: tweetsprocessed2018_02_06_8\n",
      "completed write to table: tweetsprocessed2018_02_07_1\n",
      "completed write to table: tweetsprocessed2018_02_07_10\n",
      "completed write to table: tweetsprocessed2018_02_07_2\n",
      "completed write to table: tweetsprocessed2018_02_07_3\n",
      "completed write to table: tweetsprocessed2018_02_07_4\n",
      "completed write to table: tweetsprocessed2018_02_07_5\n",
      "completed write to table: tweetsprocessed2018_02_07_6\n",
      "completed write to table: tweetsprocessed2018_02_07_7\n",
      "completed write to table: tweetsprocessed2018_02_07_8\n",
      "completed write to table: tweetsprocessed2018_02_07_9\n",
      "completed write to table: tweetsprocessed2018_02_08_1\n",
      "completed write to table: tweetsprocessed2018_02_08_2\n",
      "completed write to table: tweetsprocessed2018_02_08_3\n",
      "completed write to table: tweetsprocessed2018_02_08_4\n",
      "completed write to table: tweetsprocessed2018_02_08_5\n",
      "completed write to table: tweetsprocessed2018_02_08_6\n",
      "completed write to table: tweetsprocessed2018_02_08_7\n",
      "completed write to table: tweetsprocessed2018_02_09_1\n",
      "completed write to table: tweetsprocessed2018_02_09_2\n",
      "completed write to table: tweetsprocessed2018_02_09_3\n",
      "completed write to table: tweetsprocessed2018_02_09_4\n",
      "completed write to table: tweetsprocessed2018_02_09_5\n",
      "completed write to table: tweetsprocessed2018_02_09_6\n",
      "completed write to table: tweetsprocessed2018_02_09_7\n",
      "completed write to table: tweetsprocessed2018_02_09_8\n",
      "CPU times: user 2min 52s, sys: 17.3 s, total: 3min 9s\n",
      "Wall time: 6min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# read in each tweet file and do basic cleanup and write to postgres table\n",
    "for afile in file_list:\n",
    "    tweet_df = pd.read_csv(afile, lineterminator='\\n')\n",
    "    tweet_df = tweet_df[['id', 'tokens']]\n",
    "    tweet_df['tokens'] = tweet_df['tokens'].astype('str') \n",
    "    tweet_df['tokens'] = tweet_df['tokens'].apply(lambda x: re.sub('\\s+',' ',x) )\n",
    "    tweet_table = afile.split('/')[-1].lower()\n",
    "    tweet_table = tweet_table.replace('-', '_')\n",
    "    tweet_table = tweet_table[:-4]\n",
    "    write_results_pg(tweet_df, tweet_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3dev",
   "language": "python",
   "name": "py3dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
